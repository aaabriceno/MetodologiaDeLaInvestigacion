
@inproceedings{zheng_codegeex_2023,
	address = {Long Beach CA USA},
	title = {{CodeGeeX}: {A} {Pre}-{Trained} {Model} for {Code} {Generation} with {Multilingual} {Benchmarking} on {HumanEval}-{X}},
	isbn = {979-8-4007-0103-0},
	shorttitle = {{CodeGeeX}},
	url = {https://dl.acm.org/doi/10.1145/3580305.3599790},
	doi = {10.1145/3580305.3599790},
	abstract = {Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 8 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency ∗QZ and XX contributed equally.},
	language = {en},
	urldate = {2025-10-09},
	booktitle = {Proceedings of the 29th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Zheng, Qinkai and Xia, Xiao and Zou, Xu and Dong, Yuxiao and Wang, Shan and Xue, Yufei and Shen, Lei and Wang, Zihan and Wang, Andi and Li, Yang and Su, Teng and Yang, Zhilin and Tang, Jie},
	month = aug,
	year = {2023},
	pages = {5673--5684},
	annote = {Abrstract: Se presenta CodeGeeX un modelo multilenguaje con 13000 millones de parametros para la generacion de codigo, 
CodeGeex es de acceso publico desde el 2022, asi mismo se ha demostrado que este modelo aumenta la eficiencia en un 83,4 \%de sus usuarios. Se han creado extensiones para VScode, JetBrains  y CloudStudio, generando 8000 millones  de tokens para decenas de miles de usuarios activos cada semana.


},
	file = {PDF:C\:\\Users\\anthony\\Zotero\\storage\\YAZJY6GE\\Zheng et al. - 2023 - CodeGeeX A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X.pdf:application/pdf},
}
